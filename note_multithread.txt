P3:
Single-thread Program on Computer with Single Processor
- A program runs on a single processor
+ The program's instruction are loaded from memory
+ The instruction are executed on the processor
- Registers store information needed to execute the current instruction
+ Stack pointer
+ operand argument
+ etc
- Data is loaded from memory into processor registers as needed
- If modified, the new value is the written back to memory
_ implemented by "Time slicing"
- Each thread runs on the CPU for a short time eg
+ Thread A starts, runs for a short period, then pauses
+ Thread B starts, runs for a short period, then pauses
+ Thread C starts, runs for a short period, then pauses
+ Thread B runs again from where it left off, then pauses
+ Thread c runs again from where it left off, then pauses
- This is doen very quickly ====> The threads appear to run concurrently

Thread Scheduler
- A Scheculer controls how threads execute
+ Similar to the operating system's scheduler for processes
- Pre-emtive task switching
+ A thread can run for certain time
+ The scheduler will interupt the thread when it has used up ist time slot
+ Another thread can then run
+ The scheduler will make interupted thread "sleep"

Thread Scheduling
- Threads may start in any order
- A thread may be interupted at any time
- A thread may be restarted at any time
- While a thread is sleeping, another thread may run
+ This can run the same code as the inactive thread
+ It ca access data which it shares with the inactive thread
- The execution of the threads is interleaved
+ Thread B starts and is interupted
+ Thread A starts and is interupted
.....

Disadvantages of Time slicing
- Requires "context switch"
- Save the processor state for the current thread
+ current values of processor registers
+ Program's instruction pointer
+ etc
- Load the saved processor state for the next thread
+ Values of processor registers when stoppped, etc
+ May also have to reload the thread's instructions and data
- The processor cannot execute any instructions during a context switch

Computer with single processor and Cache
Cache
- CPUs became much faster at processing instructions
- However, RAM access speed did not improve very much
- The CPU could not process instructions during data transfers
+ The CPU spent  a lot of time waiting for RAM to respond
+ Memory latency became a problem
- Computers started using small amount of "cache" memory

Cache memory
- Cache memory
 + Physically close to the CPU
 + Stores copies of current and recently used data
 + Reduces the time spent communicating with main memory
 - Uses "static" RAM
 + This is faster than the "dynamic" RAM used in main memory
 + More expensive
 + Uses more power
 + usually much smaller than main memory
 
 Cache Fetch
 - The CPU wants to fetch data
 - Does the cache have a copy of the data?
 - Yes
 + There is a "cache hit"
 + The CPU uses that copy
 - No
 + There is a "cache miss"
 + The data is fetched from main memory
 
 Cache write
 - The CPU wants to store data
 - It writes the data to the cache
 - The data in the cache is written to main memory
 + The CPU can continue working wile this happens
 
 Cache Controller
 - The process is managed by a "cache controller"
 - The data is transferred in fixed-size blocks
 + known as "cache lines"
 + Typically 64 bytes on modern hardware
 - Each cache line relates to an address in main memory
 
 
 Computer with Muliple Processor
 - Multiple sockets
 + 2 or more processor chips in the computer
 - Multiple processor cores
 + several processors within the same silicon chip
 - Hyperthreads
 + Duplicate some of the circuitry within a processor core
 + enough to run a separeted thread, with its own execution stack
 
 Memory
 - Processor speed has increased hugely
 - Memory performance has not kept up
 - Designers had to find new ways to improve performance
 + Many more registers for storing data and code in the processor
 + Additional caches between processors and main memory
 
 Multiple Levels of Cache
 - Level 1
 + Private to each processor core
 + As close to the core as possible
 - Level 2
 + Ussally private to each core
 - Level 3
 + Shared by all the cores on the same socket
 
 Cache Controller
 - Coordinates the caches
 + the same data should have the same value in call caches
 + the same data should have the same value on all cores
 + "Cache coherency"
 
 - Monitors caches for data changes 
 + A core modifies data in its level 1 cache
 + The data is updated in the core's level 2 cache
 + The data is updated in the level 3 cache
 + The data is updated in the over cores's caches
 
 Optimization
 - Pre-fetcher
 + Looks at incoming instructions
 + Fetches data before it is needed
 - Store buffer
 + This is betwwen the core and the level 1 cache
 + Modified data is written to this buffer
 - The core can proceed to the next instruction
 + Does not need to wait for the L1 cache
 - These optimiztions provide huge improvements
 + Avoid blocking the core when there is a cache miss
 
 
 Synchronization Issues
  Synchronization Issues
 - Different threads execute on different cores
 - They may share data
 - This can cause synchronization issues
 
 Issues
 - core 1's thread modifies the shared data
 - core1 writes the new value to its store buffer
 - core2's thread wants to use the shared data
 - core 2 pre-fetches the share data, or loads it from cache
 - core 2 gets the old value
 - core 2's thread does it computation, using the old value
 - core 1's store buffer writes the new value to cache
 ___________________________________________________________________________________________
 P2
 
 System Thread Interface
 System Thread Interface
 - std::thread use the system's thread implementation
 - We may need to use the thread implementation directly
 - Some functionality is not available in standard C++
 - Thread priority
 + Give a thread higher or lower share of processor time
 - Thread affinity
 + "Pin" a thread on a specific processor core
 
Managing a Thread

Detaching a Thread
- Instead of calling join(), we can call detach()
	+ The parent thread will continue executing
	+ The child thread will run until it completes
	+ Or the program terminates
	+ Analogous to "daemon" process
- When an execution thread is detached
	+ the std::thread object is no longer associated with it
	+ The destructor will not call std::terminate()
Exception in parent thread
- The destructor are called for every object in scope
	+ Including std::thread's destructor
	+ This checks whether join() or detach() have been called
	+ If neither, it calls std::terminate()
- We must call either join() or detach() before the thread is destroyed
	+ In all paths through the code
- A better solution is to use the RAII idiom
	+ wrap the std::thread object inside a class
	+ The class's destructor calls join() on the std::thread object
- A std::thread object can only be joined once
- The joinable() member functionality
	+ Returns false
	+ If join() or detach() have already been called
	+ Or if the thread object is not associated with an execution thread
- Returns true if we need to call join()
Code using RAII class
- The destructos are called in reverse order
	+ The thread_guard's destructor is called first
	+ If necessary, it calls thr.join() and waits for the execution thread to finish
	+ The thread_guard's std::thread member is then destroyed
	+ It is not associated with exection thread
	+ Its destructor does not call std::terminate()
- This applies in normal execution	
	+ And when an exception is thrown
Stopping Threads
- Execution threads can be interrupted or stopped
	+ killed, cancelled, terminated
- In general, abruptly terminating a thread is not a good idea
- std::thread does not supporr this



Multiple Threads


- We can start multiple threads
	//start 3 threads
	std::thread thr1(hello);
	std::thread thr2(hello);
	std::thread thr3(hello);
	
	// wait for them to finish
	thr1.join();
	thr2.join();
	thr3.join();
Data Sharing Between Threads
- The Threads in a program share the same memory space
	+ It is very easy to share data between the threads
- The only requirement is that the data is visible to the thread functions
	+ global or static variable, for class member thread functions
	+ Local variable captured by lambda expressions (by reference)
- Threads interleave their execution
- Threads can interfere with each others' actions
- Modifying shared data can cause data corruption
	+ This is the main source of bugs in concurrent program												<====================
Data Race
- A "data race" occurs when:
	+ Two or more threads access the same memory loaction
	+ And at least one of the threads modifies it
	+ Potentially conflicting accesses to the same memory location
- Only safe if the threads are synchronized
	+ One thread accesses the memory location at a time
	+ The other threads have to wait until it is safe for them to access it
	+ In effect, the threads execute sequentially while they access it
- A data race causes undefined behaviour
	+ the program is not guaranteed to behace consistently
- The outcome is affecte by timing changes
	+ e.g One client clears a database table
	+ Another client inserts an entry into the same table
- A data race is special case of race condition
	+ The outcome depends on when threads are scheduled to run
	
	



 
